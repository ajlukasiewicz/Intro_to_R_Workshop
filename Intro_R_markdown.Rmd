## Developed by Rachael Cox and Alex Lukasiewicz

```{r global_options, include=FALSE}
library(knitr)
library(tidyverse)
opts_chunk$set(fig.align="center", fig.height=4, fig.width=4)
```
## Workshop: Introduction to R
### In-class worksheet

**November 15th, 2024**

Computational analyses require methods and notes to be recorded the same way you would for wet lab experiments. An excellent way to do this is via R Markdown documents. R Markdown documents are documents that combine text, R code, and R code output, and figures. They are a great way to produce self-contained and documented statistical analyses.

In this first worksheet, you will learn how to do some basic markdown editing in addition to the basic use of variables and functions in R. After you have made a change to the document, press "Knit HTML" in R Studio and see what kind of a result you get. **Note:** You may have to disable pop-ups to get this to work.


## Section 1. Embedding R code

R code embedded in R chunks will be executed and the output will be shown.
```{r section 2 variables}
# R code is embedded into this chunk
x <- 5
y <- 7
z <- x * y
z
```

Play around with some basic R code, trying the following:

1. Assign integers to variables (demonstrated in the above code block).
2. Assign some strings to variables.
3. Make a vector of strings containing your top 5 favorite foods.
4. Make a vector containing 5 random numbers.
5. Combine the two vectors you created in the previous step into one data frame.
6. Call the first column of the data frame that you create.

```{r section 1 assignments}
#your code here
```

## Section 2. Built-in functions and data sets

A function is statement internally (i.e., "under the hood") coded to perform a specific task. For instance, the `head()` function displays the first several rows of a data frame or values in a vector.

R comes with many built-in functions and data sets. Type `data()` in the console to look at a list of all available data sets. Type `?iris` in the console for more information about this specific data set. You can take a glance at the `iris` data set using the `head()` function. Run the code chunk below to test this. 

```{r section 2 functions}
# preview the first few rows a data frame
```

You can also use the `summary()` function to see the summary statistics of a data set at a glance. Try this now with the `iris` data set.
```{r section 2 summary}
# your R code here
```

You can see the column names of `iris` from the code output above. Calculate the mean of the `Petal.Length` column using the `mean()` function. Calculate the range of the `Petal.Width` column using the `range()` function. **Hint:** call the columns the same way you did in Part 2 of the worksheet.

```{r section 2 exploratory statistics}
# your R code here
```

## Section 3. Reading, writing and locating files

There are several ways to upload data into your R environment. We covered one way in Part 1 of the worksheet: manual entry. However, this is clearly not feasible for big data sets--more often, we want to read in a file containing our data. Also, we tend to modify data frames and save them to a new file. 

Try the following:

1. From the workshop folder upload 'mushrooms.csv' to the RStudio server. Use the "Upload" button in the panel on the right. 
2. Use the `read_csv()` function to read the file, and save it as a data frame called `mushrooms`.

__Important: The file name must be given to the function as a string.__

3. Use the `head()` function to preview the **first 10 rows** of the new data frame. Specify the integer as the second argument of the function.
4. Save the output of the `head()` function to a new data frame called `mushrooms_tiny`.
5. Use the `write_csv` function to write the data frame `mushrooms_tiny` to a new `.csv` file. 

**Note:** If you are coding on a local installation of R, you will have to specify a path to the location of the file or move the file to the working directory. Local installations of R do now have an "Upload" function. These concepts are covered at the end of this section.

```{r section 3 reading and writing data}
# your R code here
```

For this class, we are using a computer server where everyone has a preset working directory associated with your unique student ID number. Type `getwd()` to see the file path to your working directory. On a local installation, the output of this function might look something like `C:/Users/alex/Documents/`.

```{r section 3 get working directory}
# your R code here
```

This is the directory R will default to for reading and writing files. Ideally, for real life projects, we keep all the information we need organized into folders (aka sub-directories). More often than not, we have to tell R which sub-directory we want to read a file from or write a file to. Perform the following steps to familiarize yourself with file paths and R's perception of where files are:

1. Use the "New Folder" option in the window on the bottom right to create a new folder called "day1_data".
2. Select `mushrooms_tiny.csv` by checking the box.
3. Go to "More" > "Move..." and select the new "day1_data" folder.
4. Run `list.files()` to see all the files in the current working directory.
5. Run `list.files("day1_data")` to see the files in the new sub-directory.
6. Run #5 again, but this time specify that `full.names = TRUE` as the second argument in the function.


```{r section 3 mushroom import}
#perform import steps above
```

Clear your global environment (the broom symbol in the top right window). Read the file in the sub-directory "day1_data" using `read_csv`. The function will need the full path given by the output from the code chunk above.

```{r section 3 read in data}
# your R code here
```

---------------------------------------BREAK------------------------------------
##Section 4. Tidying data example

In the lecture slides, I show examples of how to use `pivot_longer()` and `pivot_wider()` to tidy an unruly dataset. Here is the code to do so:

```{r tidying example}
#install.packages("MASS")
#library(MASS)
#data(Sitka)

#this generates the untidy Sitka data
sitka_wide <- Sitka %>% 
  mutate(time = paste("t", Time, sep = "")) %>% 
  select (!Time) %>% 
  pivot_wider(names_from = "time", values_from = "size")

```

```{r pivot_longer}
#now lets tidy it back up! Here is the code from the slide
sitka_wide %>%
  pivot_longer(
    t152:t258, names_to = "time", values_to = "size"
  )

#bonus: if you want to make it even tidier....
sitka_wide %>%
  pivot_longer(
    t152:t258, names_to = "time", values_to = "size"
  ) %>%
  mutate(Time = gsub("t","", time)) %>%
  select(!time)


```

```{r pivot_wider}
#from the lecture example
Sitka %>%
  pivot_wider(names_from = "Time",
              values_from = "size")

```


## Section 5. Row/column addition (`mutate()`, `bind_rows()`, `left_join()`)

The raw output from many types of high-throughput molecular biology experiments comes a unique sample ID (e.g., protein ID, gene ID) and that samples raw output response. (e.g., peptide abundance via mass spectrometry experiments, mRNA abundance via RNA-seq). Often we would like to accomplish the following:

1. Add new columns that perform calculations on the raw data or contain "meta data" associated with that experiment (`mutate()`).

2. Join multiple experiments into one data frame (`bind_rows()`).

3. Import annotations to sample IDs with the hope of extracting broad trends about the behavior of biological systems in different contexts (`left_join()`).

To demonstrate these concepts, we'll use real affinity purification mass spectrometry (APMS) data sets: `frog_dnai2` and `frog_heatr2`. 

```{r message = FALSE}
## import the dnai2 pull down data set
frog_dnai2 <- read_csv("frog_apms_dnai2.csv")
head(frog_dnai2)

## import the heatr pull down data set
frog_heatr2 <- read_csv("frog_apms_heatr2.csv")
head(frog_heatr2)
```


### Section 5.1 Adding a new column

For each APMS data frame, use `mutate()` to add a new column named `bait` that contains a string naming the bait used in that experiment. For instance, the `frog_dnai2` data frame should have a new column simply reading "dnai2" for each row in the new column. 

The purpose of this is to prepare the data frames to be bound together; without this new column, we would not know observation belongs to which experiment once the data frames are combined.

```{r 5.1 mutate}
# your R code here
```

### Section 5.2 Binding two experiments together

Now, using `bind_rows()` combine the two new data frames together.

```{r 5.2 bind_rows}
# your R code here
```

Another valid way to do this is with `rbind()`; the difference is that `rbind()` requires data frames with absolutely identifical column names to ensure a clean concatenation, whereas `bind_rows()` isn't so picky.


### Section 5.3 Joining an experiment table with an annotation table

Now we have a data frame that contains protein interaction data for both dnai2 and heatr2, but the actual identifiers in the `ID` are not informative. To investigate the interacting biology, we need to join an annotation table to the APMS data frame. 

```{r message = FALSE}

# import the frog gene annotations
frog_annotations <- read_csv("frog_enog_annotations.csv")
head(frog_annotations)

```

Note that the annotation data frame has an `ID` column that matches the `ID` column in the APMS data frame. Also notice that the annotation data frame has 14292 rows, and our APMS data frame has 3356 rows. The `left_join()` function keeps all the rows from the data frame used as the "left" input, while joining in places that match the common column (`ID` in this case) to the right data frame. Use `left_join()` to combine the APMS data frame with the annotation data frame, such that the final data frame has 3356 rows and 12 columns.

```{r 5.3 left_joining}
# your R code here
```

Now each observation is connected to a *X. laevis* gene name, in addition to the gene's human cousin. Not all genes have annotations, even though *X. laevis* is considered a model organism. This is common in bioinformatics.

## Section 6. Subsetting data (`filter()`, `select()`)

In biological data analysis, we're often most interested in a specific subset of data--usually scores above some threshold (p-values, z-scores, fold changes in gene expression) or the behavior of a specific gene/protein family relative to time or different conditions..

Perform the following operations to pare down the combined APMS data frame. Use the pipe (%>%) to chain data operations together and save the results to a new data frame in a single step:

1. Filter the combined and annotated APMS data frame (generated in the previous step) to only contain observations with `PSM_zscore` > 1.6 (p ~ 0.05) to pull out proteins significantly interacting with dnai2 and heatr2 (use the `filter()` function).

2. Then, in the same line of code, pipe (`%>%`) into `select()` to extract only the columns named `ID`, `bait`, `PSM_log2fc`, `PSM_zscore`, `OG_category_annotation`.

3. Finally, again in the same line of code, pipe (`%>%`) into `arrange()` to re-sort the data frame to descend by `PSM_zscore` (i.e., highest z-scores should be at the top).

```{r 6.0 filtering and selecting}
# your R code here
```

## Section 7. Aggregating data (`group_by()`, `summarize()`, `arrange()`)

To find broad patterns in big data sets, we can use the `group_by()` and `summarize()` functions to get summary information for groups of variables. For instance, the `bait` column has two groups: dnai2 and heatr2. The `OG_category_annotations` column has 22 groups if you count the "NA" values resulting from unannotated genes. Use dataframe created in section #2 and the functions `select()` and `unique()` to look at the unique values (groups) in the columns `bait` and `OG_category_annotation` 

```{r 7.0 group_by and summarize}
# your R code here
```

When you `group_by()` a variable, you are collapsing all the information for the unique values associated with that variable. Then, we use `summarize()` to extract information associated with the collapsed information.

We want to learn the functional categories for the proteins that are most signficantly measured to be interacting with both dnai2 and heatr2. To do this, use the dataframe created in section #7 and code the following steps (as a chain of functions connected by `%>%`):

1. Use `group_by()` to group both the `bait` and `OG_category_annotation` columns
2. Use `summarize()` to create a new column called `joint_zscore` that calculates a **weighted** z-score for all the proteins in each category. The equation for this is:
    - `sum(PSM_zscore)/sqrt(length(PSM_zscore))` # sum of all z-scores divided by the number of observations per group
3. Use `arrange()` to sort the data frame by `bait` and then by `joint_zscore` to get top hits.

```{r 7.0 aggregating data}
# your R code here
```

## Section 8. If that was easy...

**Bonus challenge:** Take the sepal lengths from the `iris` data set and put them into a wide table so that is one data column per species. You might be tempted to do this with the following code, which however doesn't work. Can you figure out why? 

```{r bonus challenge}
# If you remove the # signs in the lines below you will get an error; this code doesn't work
# iris %>% 
#   select(Sepal.Length, Species) %>%
#   pivot_wider(names_from = "Species", values_from = "Sepal.Length")
```

The problem is that `pivot_wider()` does not like to put data into the same row if it isn't sure that they actually belong together. In the iris table, there is no indication which "setosa" values, for example, should go with which "versicolor" values. Therefore, `pivot_wider()` prints a warning about values that are not uniquely identified.

We can avoid this issue by adding a `row` column that is repeated among the three groups. This column simply counts rows within each group, from 1 to 50 in this particular data set. This trick forces the data from the same rows for different species into one row. (That means, rows 1 of setosa, versicolor, and virginica get combined, then rows 2, and so on.)

```{r}
# your R code here
```

(At the end, if you want, you can delete this column again:)

```{r}
# your R code here
```

